<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Svm-legion : A distributed implementation of Support Vector Machines using OpenMPI and CUDA">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>DPSVM</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/thesiddharth/dpsvm">View on GitHub</a>

          <h1 id="project_title">DPSVM</h1>
          <h2 id="project_tagline">Distributed Support Vector Machine training using OpenMPI and CUDA</h2>

			<section> <h5>
              <a href="http://thesiddharth.github.io/dpsvm/checkpoint.html"> Project Checkpoint </a> || <a href="http://thesiddharth.github.io/dpsvm/proposal.html"> Proposal </a>
            </h5> </section>
		  
            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/thesiddharth/dpsvm/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/thesiddharth/dpsvm/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h3>
<a id="summary" class="anchor" href="#summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary</h3>

<p dir="ltr">
	We have implemented a distributed and parallel <strong>Support Vector Machine</strong> training algorithm for binary classification using the <strong>OpenMPI</strong> library and the <strong>CUDA</strong> parallel programming model. We parallelize the modified Sequential Minimal Optimization algorithm used by popular tools like LIBSVM, and distribute the parallelism over <strong>GPUs</strong> in a cluster.
</p>

<h3>
<a id="background" class="anchor" href="#background" aria-hidden="true"><span class="octicon octicon-link"></span></a>Background</h3>

<p dir="ltr">
	The Support Vector Machine (SVM) is a widely used supervised machine learning algorithm, which is essentially a discriminative classifier used for linear and nonlinear classification tasks. The training of an SVM is a Quadratic Programming (QP) optimization problem, where the primal is converted to its dual form. Platt originally proposed the SMO technique for solving the dual QP problem, by breaking it down to a series of smallest possible QP problems.
</p>

<p dir="ltr">
	The SMO technique iteratively solves for a target function, by updating the Lagrangian multipliers and the error terms over the entire training dataset. Each update step involves the computation of the modified inner products of two training data input vectors (the kernel trick), which is computationally intensive over the set of all training data. We aim to implement a parallel SMO algorithm which distributes the task of training over different nodes as proposed by Cao et. al. <sup>[1]</sup>, and extend it to multiple GPU devices based on Herrero-Lopez et. al.'s <sup>[2]</sup> methodology.
</p>
<p dir="ltr">
	The parallel calculation of error terms and updates of the Lagrangian multipliers will (ideally) scale with the number of sub processes, and
	allow for the use of parallel reduction with low communication overheads for global constants. We are using OpenMPI and CUDA to create a portable, high-performance implementation that leverages distributed environments.
</p>
<br/>
<p dir="ltr" class="centeredImage">
	<img
	src="images/workflow.png" width="556px;" height="401px;"/>
</p>
<br/>

<h3>
<a id="challenge" class="anchor" href="#challenge" aria-hidden="true"><span class="octicon octicon-link"></span></a>Challenges</h3>

<p dir="ltr">
	The aspect of the problem that is most challenging is parallelizing the kernel computations in a way that <strong><em>minimizes communication</em></strong> overheads between:
</p>
<ol>
	<li dir="ltr">
		<p dir="ltr">
			Nodes in the cluster
		</p>
	</li>
	<li dir="ltr">
		<p dir="ltr">
			GPU and CPU
		</p>
	</li>
</ol>

<p dir="ltr">
	For a single GPU implementation of a parallel SVM algorithm, <sup>[2]</sup> profiles time spent on communication versus computation on various training datasets. The following trends were noted:
</p>
<ol>
	<li dir="ltr">
		<p dir="ltr">
			Datasets with a large number of features per sample and a large number of samples spent most of the GPU time on computation (the <strong><em>kernel computation</em></strong>, which is a SGEMV matrix operation).
		</p>
	</li>
	<li dir="ltr">
		<p dir="ltr">
			Datasets with a relatively smaller number of features and samples were seen to spend up to 33% of GPU time on communication, during reductions and updates across GPU tasks.
		</p>
	</li>
</ol>
<p dir="ltr">
	Since we are are shifting this to a distributed environment, we would expect to see the communication-to-computation overhead increase. However, there exist even more <strong><em>complex datasets</em></strong> <sup>[3]</sup> that we expect to exhibit low ratios in our distributed environment as well.
</p>

<h3><a id="results" class="anchor" href="#results" aria-hidden="true"><span class="octicon octicon-link"></span></a>Results</h3>

<p dir="ltr">
	We have currently implemented a version, which distributes the training over multiple GPUs using OpenMPI. This version has been profiled with popular datasets such as <strong>MNIST</strong> and <strong>Forest Covertype</strong> (Asuncion &amp; Newman, 2007), which require a large no. of iterations to converge.
</p>

<p dir="ltr" class="centeredImage">
	<img
	src="images/mnist.png" width="556px;" height="401px;"/>
</p>
<br/>

<p dir="ltr" class="centeredImage">
	<img
	src="images/covertype.png" width="556px;" height="401px;"/>
</p>
<br/>    

  <h3>
<a id="goals" class="anchor" href="#goals" aria-hidden="true"><span class="octicon octicon-link"></span></a>Goals and Deliverables</h3>

<p dir="ltr">
	<strong><em>CURRENT STATUS:</em></strong>
</p>

<ul>
	<li dir="ltr">
		<p dir="ltr">
			An OpenMPI and CUDA based, distributed, parallel SVM training implementation.
		</p>
	</li>
	<li dir="ltr">
		<p dir="ltr">
			Benchmark this implementation against the standard LibSVM implementation.
		</p>
	</li>
	<li dir="ltr">
		<p dir="ltr">
			Achieved the following training speedups over LibSVM:
		</p>
	</li>
</ul>

<div dir="ltr">
	<table class="my_table">
		<colgroup>
			<col width="143"/>
			<col width="141"/>
			<col width="101"/>
			<col width="*"/>
		</colgroup>
		<tbody>
			<tr>
				<td class="setup_column">
					<strong> <p> Setup vs. Dataset </p> </strong>
				</td>
				<td class="info_column">
					<p dir="ltr">
						<strong>Adult</strong>
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						<strong>MNIST (Even vs Odd)</strong>
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						<strong>Covertype</strong>
					</p>
				</td>
			</tr>
			<tr>
				<td class="setup_column">
					<p dir="ltr">
						<strong>REFERENCE:</strong> Speedup achieved using single Tesla C1060
						<sup>[2]</sup>
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						10.45x
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						32.79x
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						-
					</p>
				</td>
			</tr>
			<tr>
				<td class="setup_column">
					<p dir="ltr">
						<strong>REFERENCE:</strong> Speedup achieved using chunking algorithm and threeTesla C1060s and one Intel Xeon E5426 2.8GHz	quad-core CPU. <sup>[3]</sup>
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						19.38x
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						129.64x
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						Too large for LibSVM,
					</p>
					<p dir="ltr">
						Runtime: 651s
					</p>
				</td>
			</tr>
			<tr>
				<td class="setup_column">
					<p dir="ltr">
						<strong>RESULT:</strong> Our Implementation on LATEDAYS (1 Tesla K40 on every node)
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						<strong>24x</strong>
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						<strong>124x</strong> (1 GPU) - <strong>531x</strong> (10 GPUs)
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						<strong>591 sec.</strong> (1 GPU)
					</p>
				</td>
			</tr>
		</tbody>
	</table>
</div>

<p dir="ltr">
	<strong><em>Hope to achieve (by 11 May):</em></strong>
</p>

<ul>
	<li dir="ltr">
		<p dir="ltr">
			Profiling against Spark LIBLINEAR or MPI LIBLINEAR, <sup>[4]</sup>
		</p>
	</li>
	<li dir="ltr">
		<p dir="ltr">
			Extend the implementation to parallelize testing phase.
		</p>
	</li>
</ul>

  <h3>
<a id="platform" class="anchor" href="#platform" aria-hidden="true"><span class="octicon octicon-link"></span></a>Platform Choice</h3>

<p dir="ltr">
	We use the OpenMPI library and the CUDA parallel programming model on the latedays cluster. As mentioned above, this would allow us to create a portable implementation of our distributed SVM along with a high-performance GP-GPU pipeline targeted at latedays. The GPUs used are a Tesla K40 each on every node in the cluster.
</p>

  <h3>
<a id="bibliography" class="anchor" href="#bibliography" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bibliography</h3>

<p dir="ltr">
	<sup>[1]</sup> Cao, Li Juan, et al. "Parallel sequential minimal optimization for the training of support vector machines." Neural Networks, IEEE Transactions on 17.4 (2006): 1039-1049.
</p>
<p dir="ltr">
	<sup>[2]</sup> Herrero-Lopez, Sergio, John R. Williams, and Abel Sanchez. "Parallel multiclass classification using SVMs on GPUs." Proceedings of the 3rd Workshop on General-Purpose Computation on Graphics Processing Units. ACM, 2010.
</p>
<p dir="ltr">
	<sup>[3]</sup> Li, Qi, et al. "Parallel multitask cross validation for Support Vector Machine using GPU." Journal of Parallel and Distributed Computing 73.3 (2013): 293-302.
</p>
<p dir="ltr">
	<sup>[4]</sup> http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/distributed-liblinear/
</p>


</body>
</html>
