<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="DPSVM : A distributed implementation of Support Vector Machines using OpenMPI and CUDA">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>DPSVM</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/thesiddharth/dpsvm">View on GitHub</a>

          <h1 id="project_title">DPSVM</h1>
          <h2 id="project_tagline">Distributed, Parallel Support Vector Machine training using OpenMPI and CUDA</h2>

			<section> <h5>
              <a href="http://thesiddharth.github.io/dpsvm/checkpoint.html"> Project Checkpoint </a> || <a href="http://thesiddharth.github.io/dpsvm/proposal.html"> Proposal </a>
            </h5> </section>
		  
            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/thesiddharth/dpsvm/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/thesiddharth/dpsvm/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h1>
			<a id="summary" class="anchor" href="#writeup" aria-hidden="true"><span class="octicon octicon-link"></span></a>WRITE-UP</h1>


	  <h3>
<a id="summary" class="anchor" href="#summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary</h3>

<p dir="ltr">
	We have implemented a distributed and parallel <strong>Support Vector Machine</strong> training algorithm for binary classification using the <strong>OpenMPI</strong> library and the <strong>CUDA</strong> parallel programming model. Our implementation achieves the <strong>same accuracy</strong> as the ubiquitously used SVM solver LibSVM on 4 popular datasets, with speedups of up to <strong>300x</strong> against the latter, and up to <strong>10x</strong> against a few of the most highly-cited, recent papers on parallel SVMs. Our test environments include the <strong>Tesla K40m</strong> GPUs present on 10 nodes of the <strong>latedays cluster</strong> and the <strong>GTX 780</strong> GPU present on the GHC-41 machine. 
</p>

<h3>
<a id="background" class="anchor" href="#background" aria-hidden="true"><span class="octicon octicon-link"></span></a>Background</h3>

<p dir="ltr">
	The Support Vector Machine (SVM) is a widely used supervised machine learning algorithm - it is essentially a discriminative classifier used for linear and nonlinear classification tasks. The training of an SVM is a Quadratic Programming (QP) optimization problem, where the primal is converted to its dual form. In 1998, John Platt originally proposed the  Sequential Minimal Optimization (SMO) technique for solving the dual QP problem, by breaking it down to a series of smallest possible QP problems.
</p>
<p dir="ltr">
	The SMO technique iteratively solves for a target function, by updating the Lagrangian multipliers and the error terms over the entire training dataset. This algorithm, or variants of it, is used by several popular SVM solvers, including <strong>LibSVM</strong>. Our implementation uses a parallel version which distributes the task of training over different nodes, as proposed by Cao et. al. <sup>[1]</sup> and extends it to multiple GPU devices based on Herrero-Lopez et. al.'s <sup>[2]</sup> methodology.  The exact loss function, and other equations describing the dual QP problem are succinctly summarized in section 2 of Herrero-Lopez et. al.’s paper - we’ll attempt to give a high level overview of the algorithm here to show how we can benefit from parallelization. As mentioned before, this is an iterative algorithm. Each update step involves:
</p>

<ol><ol>
	<li dir="ltr">
		<p dir="ltr">
			Separating the training data points into 2 sets based on a function that involves the class of each point and the Lagrange multiplier associated with each point. The latter can be thought of as a weight, with the importance of a point in classification being directly proportional to this weight.
		</p>
	</li>
	<li dir="ltr">
		<p dir="ltr">
			Selecting one representative point from each set, based on the maximum/minimum of a classification function defined for each point.
		</p>
	</li>
	<li dir="ltr">
		<p dir="ltr">
			Updating the <em>‘weights’</em> of these two points.
		</p>
	</li>
	<li dir="ltr">
		<p dir="ltr">
			Updating the classification function of all other training points, based on the two updates made in <em> step III</em>.
		</p>
	</li>
</ol></ol>
<br/>

<p dir="ltr">
	Step III involves the computation of <strong><em>three</em></strong> inner products using the <strong><em>kernel</em></strong> trick (each involving two training data input vectors). On the other hand, step IV involves the computation of <strong><em>(2*number_of_training_points)</em></strong> such inner products - each of the selected points versus the entire training set .  However, this also exposes two potential avenues for parallelization:
</p>

<br/>
<ol><ol>
	<li dir="ltr">
		<p dir="ltr">
			The inner product and classification-function update steps can be parallelized. This involves a huge number of small calculations - particularly amenable to GPUs, and in particular BLAS functions implemented on GPUs.
		</p>
	</li>
	<li dir="ltr">
		<p dir="ltr">
			The training data itself can be distributed over several nodes, with relatively small synchronization updates needed each iteration - this is particularly amenable to message passing paradigms liken OpenMPI.
		</p>
	</li>
</ol></ol>
<br/>

<p dir="ltr" class="centeredImage">
	<img
	src="images/workflow.png" width="556px;" height="401px;"/>
</p>
<br/>

<a id="approach" class="anchor" href="#approach" aria-hidden="true"><span class="octicon octicon-link"></span></a><h3>Approach</h3>

<p dir="ltr">
	Our approach seeks to exploit both the avenues of parallelization mentioned in the previous section.
</p>
<ol><ol>
	<li dir="ltr">
		<p dir="ltr">
			Parallelizing kernel calculations using GPUs: For the kernel used to evaluate inner products, we use the <strong><em>Gaussian/Radial Basis Function (RBF) kernel</em></strong> - a very popular kernel function in SVM classification as it maps to an infinite dimensional space. The RBF kernel calculates the dot product of two input vectors x and y as follows:
		</p>
		<p dir="ltr" class="centeredImage">
			<img src="images/kernel.png"/ height='30' width='400'>
		</p>
		<p dir="ltr">
			As the equation above shows, this can be decomposed to depend on the sum of the norms of the individual vectors minus twice the ordinary vector product of the two. In step 4, as mentioned in the Background section, we need to do this for two selected points, each versus the entire training set. Now, we can’t pre-compute and store the dot product of each training vector with each other vector - this would require O(N<sup>2</sup>) storage, which won’t be available on GPUs for even moderately large datasets. We can, however, pre compute and store the norm of each vector - this requires only O(N) storage. 
		</p>
		<p dir = "ltr">
			If we do this, we only need to calculate the ||x||.||y|| terms in each update phase - this corresponds to two matrix-vector products. Here, the matrix corresponds the training set and the vector corresponds each of the two selected training points. We use <strong><em>cuBLAS</em></strong> to efficiently parallelize these two matrix-vector dot products. The resulting number-of-training-examples sized vector of updates must be used to update the classification function of all the training points. However, order does not matter now - what we essentially have are a few vectors that need to be combined into one vector. Each component can be computed in parallel. For this, we use a <strong>thrust::for_each()</strong> loop with a custom functor to compute the update.
		</p>
	</li>
	<li dir="ltr">
		<p dir="ltr">
			Distributing the training data over multiple nodes: Each successive step in the SMO algorithm involves finding two candidate points (a max/min operation), updating their weights, and doing a global update which depends only on these two points and the point being updated. Thus, we can shard our training data, locally find candidate points in each shard, and do a global reduction (another simple max/min computation) to find the global candidate points. Each node can then update its own shard in parallel. 
		</p>
		<p dir="ltr">
			This has the additional potential benefit of allowing datasets too large to fit into one GPU’s memory to be used. However, our implementation sacrifices this benefit to reduce communication overheads - we store the entire dataset at every node, so that only the indices of the global candidate points need be known by each shard, and not the full vectors. This is acceptable, since there are several complex datasets which are smaller than 500MB in size and still require millions of iterations to converge - any savings in communication overhead go a long way towards improving speedups for these. However, adapting to accommodate datasets that don’t fit in a single GPU’s memory would be fairly straightforward as well.
		</p>
		<p dir="ltr">
			We use <strong>OpenMPI</strong> to achieve the functionality mentioned in this point.
		</p>
	</li>
</ol></ol>

<a id="speedup" class="anchor" href="#speedup" aria-hidden="true"><span class="octicon octicon-link"></span></a><h5>Speed-Up Tricks</h5>

<p dir="ltr">
	Our implementation also uses a couple of neat tricks to really improve performance:
</p>
<ol><ol>
	<li dir="ltr">
		<p dir="ltr">
			It’s been observed (both in literature, and by us personally through painstaking profiling sessions) that candidate points tend to repeat themselves over short bursts - this immediately indicates that caching matrix-vector dot products should be helpful. Our approach uses an LRU cache for this purpose, the size of which can be specified as an input parameter. With a cache size of about ten lines (a line being the size of a training vector), we see a <strong><em>significant performance improvement (2x - 4x)</em></strong> on all datasets.
		</p>
	</li>
	<li dir="ltr">
		<p dir="ltr">
			Post adding the cache, further profiling showed separating the data into sets and actually selecting the candidate points (steps I and II, as mentioned in the background) was often the bottleneck. Our initial implementation for this used a <strong><em>thrust::for_each()</em></strong> loop to make the sets (using a tricky, lock-less functor), and Thrust’s special maximum and minimum reduction functions to locate the points within the sets. However, this required a separate pass over each set. Instead, we replaced this with an approach where we built only a <strong><em>single aggregated set of tuples</strong></em> - the first element of the tuple represents a point in the set 1 and the second, a point in set 2. We then wrote a custom reduction functor to find the maximum of the first tuple entry and the minimum of the second in one pass itself. This  showed a significant speedup as well, shaving a third off the times in our runs.
		</p>
	</li>
	<li dir="ltr">
		<p dir="ltr">
			We use individual CUDA streams to calculate each of the 2 matrix-vector products required (if both cache lookups miss) in each iteration - this allows CUDA to mix the two SGEMV cuBLAS operations since they are mutually exclusive.
		</p>
	</li>
	<li dir="ltr">
		<p dir="ltr">
			Our implementation also has several basic optimizations. We don’t allocate any GPU memory with loops - all allocation, CUDA stream setup and cuBLAS handle setup is done in a separate setup phase, and all destruction and deallocation is done in a separate <em>‘destroy’</em> phase. Any operation that can make use of an optimized thrust approach, does so.
		</p>
	</li>
</ol></ol>

<h5>
	<a id="classification" class="anchor" href="#classification" aria-hidden="true"><span class="octicon octicon-link"></span></a>
	Classification
</h5>
<p dir="ltr">
	One of our stretch goals for this project was to parallelize classification as well. This turned out to be a necessity, since our standard sequential approach took prohibitively long times for the larger datasets. While we haven’t spent much time optimizing this, we seem to match the parallel classification timings put forward by Herrero-Lopez et. al. in their paper. Classification essentially follows the following process:
</p>

<ul><ul>
	<li dir="ltr">
		<p dir="ltr">
			For each test point, we evaluate a classification function that depends on the kernelized dot product of the test vector with each of the training points that has a non-zero, positive weight (i.e. those points of the training corpus that are <em>‘support vectors’</em>).
		</p>
	</li>
	<li dir="ltr">
		<p dir="ltr">
			We then add an unregularized bias term that is calculated as a result of the training process.
		</p>
	</li>
	<li dir="ltr">
		<p dir="ltr">
			The sign of the final summation gives us our class prediction.
		</p>
	</li>
</ul></ul>


<p dir="ltr">
	Our approach involves using a cuBLAS SGEMV operation, followed by a thrust transform_reduce operation using a custom functor to achieve this. For getting training accuracy (i.e. classifying the training data immediately post training), we also use Thrust to sparsify the training matrix by removing those points that aren’t support vectors in parallel.
</p>
<br/>

<h3>
	<a id="results" class="anchor" href="#results" aria-hidden="true"><span class="octicon octicon-link"></span></a>
	Results
</h3>

<p dir="ltr">
	We've reused the CycleTimer we used in our homework assignments to measure times, both for overall run-timings, and for profiling the time taken by individual program segments.

	First off, this is the table of results for the 3 datasets mentioned in our proposal. All speedups mentioned are versus LibSVM runs on an Intel Core i7-920 2.67 GHz CPU. These LibSVM timings are reported by Herrero-Lopez et. al. and are the quickest ones amongst the various papers we referred to.
</p>

<div dir="ltr">
	<table class="my_table">
		<colgroup>
			<col width="143"/>
			<col width="141"/>
			<col width="101"/>
			<col width="*"/>
		</colgroup>
		<tbody>
			<tr>
				<td class="setup_column">
					<strong> <p> Setup vs. Dataset </p> </strong>
				</td>
				<td class="info_column">
					<p dir="ltr">
						<strong>Adult</strong>
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						<strong>MNIST (Even vs Odd)</strong>
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						<strong>Covertype</strong>
					</p>
				</td>
			</tr>
			<tr>
				<td class="setup_column">
					<p dir="ltr">
						<strong>REFERENCE:</strong> Speedup achieved using a single Tesla C1060.
						<sup>[2]</sup>
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						10.45x
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						32.79x
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						-
					</p>
				</td>
			</tr>
			<tr>
				<td class="setup_column">
					<p dir="ltr">
						<strong>REFERENCE:</strong> Speedup achieved using a less-accurate, quicker chunking algorithm on three Tesla C1060s. <sup>[3]</sup>
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						13.53x
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						82.9x
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						(Note: 500,000 point subset of training data used) <br>
						Too large for LibSVM, <br>
						Runtime: 651 s.
					</p>
				</td>
			</tr>
			<tr>
				<td class="setup_column">
					<p dir="ltr">
						<strong>RESULT:</strong> Our Implementation: Single GPU calculations on GTX 780, multi-GPU calculations on latedays: 1 Tesla K40 on every node. CPU-based LibSVM times taken from [3] 
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						<strong>14.23x</strong> (1 GPU)
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						<strong>102x</strong> (1 GPU) - <strong>304x</strong> (10 GPUs)
					</p>
					<p dir="ltr">
						<em>(in ranges of 500x if tolerance is reduced)</em>
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						(Note: Full 581,012 point dataset used.) <br>
						<strong>591 s.</strong> (10 GPUs)
					</p>
				</td>
			</tr>
		</tbody>
	</table>
</div>

<p dir="ltr">
	The graphs below summarize the runtime and the speed-up statistics of DPSVM with respect to Herrero-Lopez et. al.'s implementation, for the <strong>MNIST dataset</strong> (LeCun et al., 1998) and the <strong>Web dataset</strong> (Platt, 1999). As mentioned in the table above, Herrero-Lopez et. al. profile their implementation using an <strong>NVIDIA Tesla C1060 GPU</strong>, while we use an <strong>NVIDIA GTX 780</strong> for the single node case, and the <strong>NVIDIA Tesla K40m</strong> GPUs on latedays for the multiple node case.
</p>

<p dir="ltr">
	The <strong>Spark MLLib's SVMWithSGD()</strong> implementation was profiled on an <strong>AWS EMR</strong> cluster, using <strong>10 m3.2xlarge</strong> machines. Each m3.2xlarge machine has an Intel Xeon E5-2670 v2 CPU, with 8 vCPUs and 30GB of memory. The RDDs were persisted in memory for this Spark implementation. SVMWithSGD() essentially acts like a <strong><em>linear</em></strong> kernel, which would have smaller run-times, but sacrifices accuracy of classification for datasets which can't be separated with linear hyperplanes.
</p>

<div>
	<img src="images/mnist_1.png" width="400px;" height="300px;"/>
	<img src="images/web_1.png" width="400px;" height="300px;"/>
</div>
<br/>

<div>
	<img src="images/mnist_3.png" width="400px;" height="300px;"/>
	<img src="images/web_3.png" width="400px;" height="300px;"/>
</div>
<br/> 

<div>
	<img src="images/mnist_4.png" width="400px;" height="300px;"/>
	<img src="images/web_4.png" width="400px;" height="300px;"/>
</div>
<br/>    

<h5>
	<a id="challenge" class="anchor" href="#challenge" aria-hidden="true"><span class="octicon octicon-link"></span></a>
	Challenges
</h5>

	Since we were using a distributed environment, we expected to see the communication-to-computation overhead increase with node-count for smaller datasets, and weren't wrong: this trend was observed using the <strong>Adult dataset</strong> (Asuncion &amp; Newman, 2007), which actually performs better on a single GPU, as opposed to distributing work across different GPUs. This has been confirmed by profiling the run-time of the runs, with breakdown of the communication and computation times. The next few graphs illustrate this trend.
</p>

<div>
	<img src="images/adult_1.png" width="400px;" height="300px;"/>
	<img src="images/adult_2.png" width="400px;" height="300px;"/>
</div>
<br/>

<div>
	<img src="images/adult_3.png" width="400px;" height="300px;"/>
	<img src="images/adult_4.png" width="400px;" height="300px;"/>
</div>

<p>As a side note, we'd also like to talk about the disparity in the hardware used in the reported statistics. This stems from several issues - one, to the best of our knowledge, there isn't any pre-existing distributed AND parallel implementation to benchmark against. Secondly, while GPU-optimized implementations are available online - they are not packaged well. We are still trying to resolve the dependencies for a couple of online implementations we recently found. Finally, a lot of the published results in this area are circumspect in the testing criteria they report, and the modifications they make to the datasets they test on. For example, some papers we found do not report the tolerance they used to converge - this has a direct bearing on accuracy, training time and number of support vectors identified.    <br/>

In our results, we've tried to report the most reliable and highly-cited results we could could find. Additionally, for further reference, we've made a couple of last minute runs on the lowest performing GPU available on the GHC-clusters (the GPU 480X), and present those results here as well:
 </p>
 <div dir="ltr">
	<table class="my_table">
		<colgroup>
			<col width="143"/>
			<col width="141"/>
			<col width="101"/>
			<col width="*"/>
		</colgroup>
		<tbody>
			<tr>
				<td class="setup_column">
					<strong> <p> Setup vs. Dataset </p> </strong>
				</td>
				<td class="info_column">
					<p dir="ltr">
						<strong>Adult</strong>
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						<strong>MNIST (Even vs Odd)</strong>
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						<strong>Web</strong>
					</p>
				</td>
			</tr>
			<tr>
				<td class="setup_column">
					<p dir="ltr">
						<strong>REFERENCE:</strong> Speedup achieved using a single Tesla C1060.
						<sup>[2]</sup>
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						32.67s
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						425.89s
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						156.95s
					</p>
				</td>
			</tr>
			<tr>
				<td class="setup_column">
					<p dir="ltr">
						<strong>REFERENCE:</strong> Speedup achieved using a less-accurate, quicker chunking algorithm on three Tesla C1060s. <sup>[3]</sup>
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						25.23s
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						168.42s
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						78.40s
					</p>
				</td>
			</tr>
			<tr>
				<td class="setup_column">
					<p dir="ltr">
						<strong>RESULT:</strong> Our Implementation: Single GPU calculations on GTX 480 
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						<strong>31s</strong>
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						<strong>208s</strong> 
					</p>
				</td>
				<td class="info_column">
					<p dir="ltr">
						<strong>88s</strong>
					</p>
				</td>
			</tr>
		</tbody>
	</table>
</div>

Even here, our results are significantly better than the single C1060 implementation, and are fairly close to the three-C1060 implementation of a simpler, less-accurate training algorithm. We feel these results serve as a good stand-in, until we are able to either procure equivalent hardware to test our implementation on, or get one of the cited implementations running on our own hardware. Additionally, we'd also like to maintain the focus on our USP, the distribution of parallelism over clusters, which can lead to significant improvement in run-times. This can clearly be seen in runs on the complex forest cover-type dataset <sup>[6]</sup>. This dataset consists of 581012 data points, each with 54 attributes, and requires around a million iterations to converge with RBF kernel parameters: <em>C</em> = 10, <em>gamma</em> = 0.125, and <em>tolerance</em> = 0.01.

<div class="centeredImage">
	<img src="images/covertype_new.png" width="400px;" height="300px;"/>
</div>

<h5>
	<a id="runs" class="anchor" href="#runs" aria-hidden="true"><span class="octicon octicon-link"></span></a>
	Precise Run Statistics
</h5>

<strong>MNIST</strong>
<br/>
Training points: 60,000 ; Testing points: 10,000 <br/>
<strong>Parameters of Gaussian Kernel: <em>C</em></strong> = 10, <strong><em>gamma</em></strong> = 0.125, <strong><em>tolerance</em></strong> = 0.001
<br/>

<div dir="ltr">
	<table>
    <tr>
        <td><strong>Number of nodes</strong></td>
        <td><strong>LibSVM 1</strong></td>
        <td><strong>Lopez et. al. 1 GPU</strong></td>
        <td><strong>1</strong></td>
        <td><strong>2</strong></td>
        <td><strong>4</strong></td>
        <td><strong>6</strong></td>
        <td><strong>8</strong></td>
        <td><strong>10</strong></td>
        <td><strong>Spark 10</strong></td>
    </tr>
    <tr>
        <td><strong>Svm training time (in sec.)</strong></td>
        <td>13964</td>
        <td>426</td>
        <td>137</td>
        <td>106</td>
        <td>66</td>
        <td>54</td>
        <td>48</td>
        <td>46</td>
        <td>101</td>
    </tr>
    <tr>
        <td><strong>MPI communication time (in sec.)</strong></td>
        <td>n/a</td>
        <td>n/a</td>
        <td>0</td>
        <td>4</td>
        <td>9</td>
        <td>7</td>
        <td>15</td>
        <td>18</td>
        <td>n/a</td>
    </tr>
    <tr>
        <td><strong>Number of iterations to converge</strong></td>
        <td>76104</td>
        <td>68038</td>
        <td>68200</td>
        <td>67995</td>
        <td>67866</td>
        <td>67966</td>
        <td>67884</td>
        <td>68037</td>
        <td>n/a</td>
    </tr>
    <tr>
        <td><strong>Number of support vectors</strong></td>
        <td>43753</td>
        <td>43731</td>
        <td>43732</td>
        <td>43731</td>
        <td>43730</td>
        <td>43730</td>
        <td>43729</td>
        <td>43728</td>
        <td>n/a</td>
    </tr>
    <tr>
        <td><strong>b value (intercept)</strong></td>
        <td>--</td>
        <td>--</td>
        <td>-0.249805</td>
        <td>-0.249805</td>
        <td>-0.249864</td>
        <td>-0.249809</td>
        <td>-0.249868</td>
        <td>-0.249799</td>
        <td>n/a</td>
    </tr>
    <tr>
        <td><strong>Test accuracy</strong></td>
        <td>0.9532</td>
        <td>0.9532</td>
        <td>0.9532</td>
        <td>0.9535</td>
        <td>0.9535</td>
        <td>0.9535</td>
        <td>0.9535</td>
        <td>0.9535</td>
        <td>0.878</td>
    </tr>
</table>
</div>

<br/>

<strong>Web</strong>
<br/>
Training points: 49,749 ; Testing points: 10,000 <br/>
<strong>Parameters of Gaussian Kernel: <em>C</em></strong> = 64, <strong><em>gamma</em></strong> = 7.8125, <strong><em>tolerance</em></strong> = 0.001
<br/>

<div dir="ltr">
	<table>
    <tr>
        <td><strong>Number of nodes</strong></td>
        <td><strong>LibSVM 1</strong></td>
        <td><strong>Lopez et. al. 1 GPU</strong></td>
        <td><strong>1</strong></td>
        <td><strong>2</strong></td>
        <td><strong>4</strong></td>
        <td><strong>6</strong></td>
        <td><strong>8</strong></td>
        <td><strong>10</strong></td>
        <td><strong>Spark 10</strong></td>
    </tr>
    <tr>
        <td><strong>Svm training time (in sec.)</strong></td>
        <td>2350</td>
        <td>156.95</td>
        <td>60</td>
        <td>49</td>
        <td>38</td>
        <td>41</td>
        <td>34</td>
        <td>36</td>
        <td>22</td>
    </tr>
    <tr>
        <td><strong>MPI communication time (in sec.)</strong></td>
        <td>n/a</td>
        <td>n/a</td>
        <td>0</td>
        <td>6</td>
        <td>12</td>
        <td>20</td>
        <td>14</td>
        <td>14</td>
        <td>n/a</td>
    </tr>
    <tr>
        <td><strong>Number of iterations to converge</strong></td>
        <td>85299</td>
        <td>76242</td>
        <td>77886</td>
        <td>79953</td>
        <td>80738</td>
        <td>87146</td>
        <td>78507</td>
        <td>75968</td>
        <td>n/a</td>
    </tr>
    <tr>
        <td><strong>Number of support vectors</strong></td>
        <td>35232</td>
        <td>35220</td>
        <td>35220</td>
        <td>35220</td>
        <td>35220</td>
        <td>35220</td>
        <td>35220</td>
        <td>35220</td>
        <td>n/a</td>
    </tr>
    <tr>
        <td><strong>b value (intercept)</strong></td>
        <td>--</td>
        <td>--</td>
        <td>0.948853</td>
        <td>0.948856</td>
        <td>0.949773</td>
        <td>0.946923</td>
        <td>0.948893</td>
        <td>0.948896</td>
        <td>n/a</td>
    </tr>
    <tr>
        <td><strong>Test accuracy</strong></td>
        <td>0.99415</td>
        <td>0.99415</td>
        <td>0.9919</td>
        <td>0.994449</td>
        <td>0.994449</td>
        <td>0.994449</td>
        <td>0.994449</td>
        <td>0.994449</td>
        <td>0.9709049562</td>
    </tr>
</table>
</div>

<strong>Adult</strong>
<br/>
Training points: 32,561 ; Testing points: 16,281 <br/>
<strong>Parameters of Gaussian Kernel: <em>C</em></strong> = 100, <strong><em>gamma</em></strong> = 0.5, <strong><em>tolerance</em></strong> = 0.001
<br/>

<div dir="ltr">
	<table>
    <tr>
        <td><strong>Number of nodes</strong></td>
        <td><strong>LibSVM 1</strong></td>
        <td><strong>Lopez et. al. 1 GPU</strong></td>
        <td><strong>1</strong></td>
        <td><strong>2</strong></td>
        <td><strong>4</strong></td>
        <td><strong>6</strong></td>
        <td><strong>8</strong></td>
        <td><strong>10</strong></td>
        <td><strong>Spark 10</strong></td>
    </tr>
    <tr>
        <td><strong>Svm training time (in sec.)</strong></td>
        <td>341.5</td>
        <td>32.67</td>
        <td>24</td>
        <td>33</td>
        <td>37</td>
        <td>41</td>
        <td>43</td>
        <td>48</td>
        <td>15</td>
    </tr>
    <tr>
        <td><strong>MPI communication time (in sec.)</strong></td>
        <td>n/a</td>
        <td>n/a</td>
        <td>0</td>
        <td>7</td>
        <td>15</td>
        <td>16</td>
        <td>17</td>
        <td>17</td>
        <td>n/a</td>
    </tr>
    <tr>
        <td><strong>Number of iterations to converge</strong></td>
        <td>43735</td>
        <td>115177</td>
        <td>115612</td>
        <td>131555</td>
        <td>131422</td>
        <td>130858</td>
        <td>132037</td>
        <td>131207</td>
        <td>n/a</td>
    </tr>
    <tr>
        <td><strong>Number of support vectors</strong></td>
        <td>19058</td>
        <td>18677</td>
        <td>18694</td>
        <td>18718</td>
        <td>18705</td>
        <td>18705</td>
        <td>18708</td>
        <td>18689</td>
        <td>n/a</td>
    </tr>
    <tr>
        <td><strong>b value (intercept)</strong></td>
        <td>--</td>
        <td>--</td>
        <td>0.469509</td>
        <td>0.455351</td>
        <td>0.472694</td>
        <td>0.478415</td>
        <td>0.477845</td>
        <td>0.497827</td>
        <td>n/a</td>
    </tr>
    <tr>
        <td><strong>Test accuracy</strong></td>
        <td>0.82697</td>
        <td>0.82697</td>
        <td>0.82722</td>
        <td>0.827283</td>
        <td>0.827283</td>
        <td>0.827283</td>
        <td>0.827283</td>
        <td>0.827283</td>
        <td>0.8392</td>
    </tr>
</table>
</div>
<h5>
	<a id="platform" class="anchor" href="#platform" aria-hidden="true"><span class="octicon octicon-link"></span></a>
	Platform Choice
</h5>

<p dir="ltr">
	After all our testing, we feel our results confirm that our distributed-GPU approach was a sound choice. Each iteration step is composed of a huge number of small calculations - amenable to GPUs. For complex datasets, communication overhead between nodes is outweighed by the lower iteration time on smaller, sharded data.
</p>

<h5>
	<a id="improve" class="anchor" href="#improve" aria-hidden="true"><span class="octicon octicon-link"></span></a>
	Room to improve
</h5>

<p dir="ltr">
	We feel we still have quite a few avenues open for improvement: we want to make some changes that will remove CPU-GPU communication entirely and rely on OpenMPI 1.8's neat GPU-GPU buffer transfer feature. Integrating this efficiently with thrust will be pretty tricky. Additionally, we'd also like to update our algorithm to use an adaptive approach that switches between the first-order heuristic we currently use (which has a large number of quick iterations), and a second-order heuristic used in some implementations (which has a smaller number of longer iterations). Finally, we'd also like to add support for more kernels. We hope to get some of this done over the summer.
</p>

<h5>
	<a id="bottomline" class="anchor" href="#bottomline" aria-hidden="true"><span class="octicon octicon-link"></span></a>
	The Bottom Line
</h5>

<p dir="ltr">
	We've come up with a parallel SVM training module which is as accurate as LibSVM, while being quicker than most other current available implementations. When shifted to a distributed environment, we feel our implementation would be hard to beat in terms of runtime for complex datasets. Additionally, we're confident the interface we provide will seem intuitive to any user who is familiar with LibSVM, or any other prominent sequential package available today.
</p>

  <h3>
<a id="bibliography" class="anchor" href="#bibliography" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bibliography</h3>

<p dir="ltr">
	<sup>[1]</sup> Cao, Li Juan, et al. "Parallel sequential minimal optimization for the training of support vector machines." Neural Networks, IEEE Transactions on 17.4 (2006): 1039-1049.
</p>
<p dir="ltr">
	<sup>[2]</sup> Herrero-Lopez, Sergio, John R. Williams, and Abel Sanchez. "Parallel multiclass classification using SVMs on GPUs." Proceedings of the 3rd Workshop on General-Purpose Computation on Graphics Processing Units. ACM, 2010.
</p>
<p dir="ltr">
	<sup>[3]</sup> Li, Qi, et al. "Parallel multitask cross validation for Support Vector Machine using GPU." Journal of Parallel and Distributed Computing 73.3 (2013): 293-302.
</p>
<p dir="ltr">
	<sup>[4]</sup> http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/distributed-liblinear/
</p>
<p dir="ltr">
	<sup>[5]</sup> B. Catanzaro, N. Sundaram, and K. Keutzer. Fast support vector machine training and classification on graphics processors. In ICML ’08: Proceedings of the 25th international conference on Machine learning, pages 104–111, New York, NY, USA, 2008. ACM.
</p>
<p dir="ltr">
	<sup>[6]</sup>
Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science
</p>

<h3>
	<a id="work" class="anchor" href="#work" aria-hidden="true"><span class="octicon octicon-link"></span></a>
	Work division
</h3>
<p dir="ltr">
	Equal work was performed by both project members.
</p>

</body>
</html>
